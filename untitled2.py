# -*- coding: utf-8 -*-
"""Untitled2.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1xcMoEEb4l4jEbE9Q-06L_t50qCqMopBK
"""

# StudyMate - Complete Working Version with IBM Granite 3.3 2B
# Install dependencies first:
# !pip install -q pymupdf sentence-transformers faiss-cpu gradio transformers python-docx fpdf gTTS torch accelerate

import os, re, json, tempfile, uuid
from typing import List, Dict, Any, Tuple, Optional
import fitz  # PyMuPDF
import numpy as np
from sentence_transformers import SentenceTransformer
import faiss
import gradio as gr
from gtts import gTTS
from datetime import datetime
from docx import Document
from fpdf import FPDF
import torch
from transformers import AutoTokenizer, AutoModelForCausalLM

# ==================== CONFIGURATION ====================
print("ğŸ”„ Loading models...")

# Embedding model
EMBED_MODEL = SentenceTransformer("sentence-transformers/all-MiniLM-L6-v2")
print("âœ… Embedding model loaded!")

# IBM Granite model
GRANITE_MODEL_NAME = "ibm-granite/granite-3.0-2b-instruct"
granite_tokenizer = None
granite_model = None

try:
    print("ğŸ”„ Loading IBM Granite model...")
    granite_tokenizer = AutoTokenizer.from_pretrained(GRANITE_MODEL_NAME)
    granite_model = AutoModelForCausalLM.from_pretrained(
        GRANITE_MODEL_NAME,
        device_map="auto",
        torch_dtype=torch.float16,
        trust_remote_code=True
    )
    print("âœ… IBM Granite model loaded!")
except Exception as e:
    print(f"âš ï¸ Granite model loading error: {e}")
    print("Will use fallback mode")

# Global variables
VECTOR_INDEX = None
METADATA: List[Dict[str, Any]] = []
DOC_LIST: List[str] = []

# ==================== CORE FUNCTIONS ====================

def extract_text_from_pdf(pdf_file, filename: str) -> List[Dict[str,Any]]:
    """Extract text chunks from PDF with page numbers."""
    try:
        if isinstance(pdf_file, str):
            doc = fitz.open(pdf_file)
        else:
            pdf_bytes = pdf_file.read() if hasattr(pdf_file, 'read') else pdf_file
            doc = fitz.open(stream=pdf_bytes, filetype="pdf")

        chunks = []
        total_pages = len(doc)
        print(f"ğŸ“„ Processing {filename}: {total_pages} pages")

        for pno in range(total_pages):
            page = doc.load_page(pno)
            text = page.get_text("text")
            text = re.sub(r'\s+', ' ', text).strip()

            if not text or len(text) < 20:
                continue

            sentences = re.split(r'(?<=[.!?])\s+(?=[A-Z])', text)

            current_chunk = ""
            for sent in sentences:
                sent = sent.strip()
                if len(current_chunk) + len(sent) < 500:
                    current_chunk += " " + sent
                else:
                    if len(current_chunk.strip()) > 40:
                        chunks.append({
                            'text': current_chunk.strip(),
                            'page': pno + 1,
                            'source': filename,
                            'chunk_id': str(uuid.uuid4())[:8]
                        })
                    current_chunk = sent

            if len(current_chunk.strip()) > 40:
                chunks.append({
                    'text': current_chunk.strip(),
                    'page': pno + 1,
                    'source': filename,
                    'chunk_id': str(uuid.uuid4())[:8]
                })

        doc.close()
        print(f"âœ… Extracted {len(chunks)} chunks from {filename}")
        return chunks

    except Exception as e:
        print(f"âŒ Error processing {filename}: {str(e)}")
        return []

def index_documents(files) -> str:
    """Index uploaded PDF files."""
    global VECTOR_INDEX, METADATA, DOC_LIST

    if not files:
        return "âŒ No files uploaded. Please upload PDF files first."

    print(f"\nğŸ”„ Starting indexing for {len(files)} file(s)...")

    METADATA = []
    texts = []
    DOC_LIST = []

    for file_obj in files:
        try:
            if hasattr(file_obj, 'name'):
                fname = os.path.basename(file_obj.name)
            else:
                fname = "document.pdf"

            print(f"\nğŸ“– Processing: {fname}")
            DOC_LIST.append(fname)

            chunks = extract_text_from_pdf(file_obj, fname)

            if not chunks:
                print(f"âš ï¸ No text from {fname}")
                continue

            for chunk in chunks:
                METADATA.append(chunk)
                texts.append(chunk['text'])

            print(f"âœ… Added {len(chunks)} chunks")

        except Exception as e:
            print(f"âŒ Error: {str(e)}")
            continue

    if not texts:
        return "âŒ No text found in PDFs. Check if PDFs contain readable text."

    try:
        print(f"\nğŸ”„ Building index for {len(texts)} chunks...")
        embeddings = EMBED_MODEL.encode(texts, show_progress_bar=True, convert_to_numpy=True)
        faiss.normalize_L2(embeddings)

        idx = faiss.IndexFlatIP(embeddings.shape[1])
        idx.add(embeddings)
        VECTOR_INDEX = idx

        print("âœ… Index created!")

        return f"""âœ… **Indexing Complete!**

ğŸ“š Documents: {len(DOC_LIST)}
ğŸ“„ Pages: {len(set(m['page'] for m in METADATA))}
ğŸ”¢ Chunks: {len(texts)}

**Files:**
{chr(10).join(['â€¢ ' + doc for doc in DOC_LIST])}"""

    except Exception as e:
        return f"âŒ Error building index: {str(e)}"

def search_documents(query: str, top_k: int = 5) -> List[Tuple[Dict[str,Any], float]]:
    """Semantic search."""
    global VECTOR_INDEX, METADATA

    if VECTOR_INDEX is None or len(METADATA) == 0:
        return []

    try:
        q_emb = EMBED_MODEL.encode([query], convert_to_numpy=True)
        faiss.normalize_L2(q_emb)
        D, I = VECTOR_INDEX.search(q_emb, min(top_k, len(METADATA)))

        results = []
        for score, idx in zip(D[0], I[0]):
            if 0 <= idx < len(METADATA):
                results.append((METADATA[idx], float(score)))

        return results
    except Exception as e:
        print(f"Search error: {e}")
        return []

# ==================== IBM GRANITE INTEGRATION ====================

def call_granite_model(prompt: str, system_prompt: str, max_tokens: int = 1024) -> str:
    """Call IBM Granite model."""

    if granite_model is None or granite_tokenizer is None:
        return "âš ï¸ Granite model not loaded. Using fallback response.\n\n" + prompt[:500]

    try:
        messages = [
            {"role": "system", "content": system_prompt},
            {"role": "user", "content": prompt}
        ]

        chat_template = granite_tokenizer.apply_chat_template(
            messages,
            tokenize=False,
            add_generation_prompt=True
        )

        inputs = granite_tokenizer(chat_template, return_tensors="pt").to(granite_model.device)

        with torch.no_grad():
            outputs = granite_model.generate(
                **inputs,
                max_new_tokens=max_tokens,
                temperature=0.7,
                top_p=0.9,
                do_sample=True,
                pad_token_id=granite_tokenizer.eos_token_id
            )

        response = granite_tokenizer.decode(outputs[0][inputs['input_ids'].shape[1]:], skip_special_tokens=True)
        return response.strip()

    except Exception as e:
        print(f"Granite error: {e}")
        return f"Error: {str(e)}\n\nFallback: {prompt[:500]}"

# ==================== FEATURE FUNCTIONS ====================

def answer_question(question: str, difficulty: str, system_prompt: str) -> Tuple[str, str, int]:
    """Answer questions using RAG with Granite."""

    if not question.strip():
        return "â“ Please enter a question.", "", 0

    if not VECTOR_INDEX:
        return "ğŸ“š **No documents indexed!**\n\nGo to 'Upload Documents' tab first.", "", 0

    results = search_documents(question, top_k=5)

    if not results:
        return "âŒ No relevant information found.", "", 0

    context_parts = []
    citations = []

    for i, (meta, score) in enumerate(results):
        context_parts.append(f"[Source {i+1}] {meta['text']}")
        citations.append(f"""**ğŸ“„ Source {i+1}** (Relevance: {score:.1%})
â€¢ File: {meta['source']}
â€¢ Page: {meta['page']}
â€¢ Content: "{meta['text'][:200]}..."
""")

    context = "\n\n".join(context_parts)
    citations_text = "\n".join(citations)

    difficulty_map = {
        "Beginner": "Explain in very simple terms with examples. Avoid jargon.",
        "Intermediate": "Provide balanced explanation with some technical details.",
        "Advanced": "Give detailed technical explanation with advanced concepts."
    }

    full_system = f"""{system_prompt}

{difficulty_map.get(difficulty, "")}

Format response with:
# Headings
â€¢ Bullet points
â€¢ Code blocks with ```
â€¢ Step-by-step when needed"""

    prompt = f"""Context from documents:
{context}

Question: {question}

Provide a detailed, well-formatted answer based ONLY on the context above."""

    answer = call_granite_model(prompt, full_system, max_tokens=1024)

    confidence = int(results[0][1] * 100) if results else 0

    formatted_answer = f"""# ğŸ“ Answer

{answer}

---
*Model: IBM Granite 3.0 2B | Confidence: {confidence}% | Sources: {len(results)}*"""

    return formatted_answer, citations_text, confidence

def generate_notes(topic: str, system_prompt: str) -> str:
    """Generate study notes."""

    if not topic.strip():
        return "â“ Please enter a topic."

    if not VECTOR_INDEX:
        return "ğŸ“š No documents indexed."

    results = search_documents(topic, top_k=8)
    if not results:
        return "âŒ No relevant information found."

    context = "\n\n".join([f"[{i+1}] {meta['text']}" for i, (meta, _) in enumerate(results)])

    prompt = f"""Create comprehensive study notes on: {topic}

Context from documents:
{context}

Format with:
# Main Topic
## Subtopics
â€¢ Key points
â€¢ Examples
â€¢ Definitions"""

    notes = call_granite_model(prompt, system_prompt, max_tokens=1500)

    sources = set([meta['source'] for meta, _ in results])
    notes += f"\n\n---\n**ğŸ“š Sources:** {', '.join(sources)}"

    return notes

def generate_quiz(topic: str, num_questions: int, system_prompt: str) -> str:
    """Generate quiz."""

    if not topic.strip():
        return "â“ Please enter a topic."

    if not VECTOR_INDEX:
        return "ğŸ“š No documents indexed."

    results = search_documents(topic, top_k=10)
    if not results:
        return "âŒ No relevant information found."

    context = "\n\n".join([meta['text'] for meta, _ in results[:num_questions]])

    prompt = f"""Generate {num_questions} multiple-choice questions about: {topic}

Context:
{context}

Format each:
**Q1. [Question]**
A) Option A
B) Option B
C) Option C
D) Option D
âœ“ Correct: [Letter]
ğŸ“ Explanation: [Why]
"""

    quiz = call_granite_model(prompt, f"{system_prompt}\n\nCreate clear educational quiz questions.", max_tokens=1500)

    return quiz

def generate_flashcards(topic: str, num_cards: int) -> str:
    """Generate flashcards."""

    if not topic.strip():
        return "â“ Please enter a topic."

    if not VECTOR_INDEX:
        return "ğŸ“š No documents indexed."

    results = search_documents(topic, top_k=num_cards)
    if not results:
        return "âŒ No relevant information found."

    cards = f"# ğŸƒ Flashcards: {topic}\n\n"

    for i, (meta, score) in enumerate(results[:num_cards], 1):
        text = meta['text']
        sentences = text.split('.')

        front = sentences[0].strip()[:150] if sentences else text[:150]
        back = '. '.join(sentences[1:3]).strip()[:300] if len(sentences) > 1 else text[:300]

        cards += f"""---
### Card {i}

**ğŸ”¹ Front:**
{front}

**ğŸ”¹ Back:**
{back}

*Source: {meta['source']}, Page {meta['page']}*

"""

    return cards

def smart_pdf_search(query: str) -> str:
    """Smart search."""

    if not METADATA:
        return "ğŸ“š No documents indexed."

    query_lower = query.lower()
    results = []

    for meta in METADATA:
        text = meta['text']
        matched = False
        match_type = "Keyword"

        if 'formula' in query_lower or 'equation' in query_lower:
            if any(c in text for c in ['=', 'âˆ‘', 'âˆ«', 'âˆ‚']):
                matched = True
                match_type = "Formula"
        elif 'table' in query_lower:
            if re.search(r'\btable\b', text, re.I):
                matched = True
                match_type = "Table"
        elif 'definition' in query_lower:
            if re.search(r'\bdefined as\b|\bdefinition\b', text, re.I):
                matched = True
                match_type = "Definition"
        elif query_lower in text.lower():
            matched = True

        if matched:
            results.append(f"""**ğŸ“„ {meta['source']}** (Page {meta['page']}) - *{match_type}*
> {text[:300]}...
""")

    if not results:
        return f"âŒ No matches for '{query}'."

    return f"# ğŸ” Search: {query}\n\nFound {len(results)}:\n\n" + "\n".join(results[:15])

def create_tts(text: str, lang: str):
    """Generate TTS."""
    if not text.strip():
        return None
    try:
        tts = gTTS(text=text[:1000], lang=lang)
        tmp = tempfile.NamedTemporaryFile(delete=False, suffix=".mp3")
        tts.save(tmp.name)
        return tmp.name
    except:
        return None

def export_notes_docx(text: str):
    """Export to DOCX."""
    if not text.strip():
        return None
    try:
        doc = Document()
        doc.add_heading('StudyMate Notes', 0)
        doc.add_paragraph(text)
        path = os.path.join(tempfile.gettempdir(), f"Notes_{datetime.now().strftime('%Y%m%d_%H%M%S')}.docx")
        doc.save(path)
        return path
    except:
        return None

def get_statistics() -> str:
    """Get stats."""
    if not METADATA:
        return "ğŸ“Š No documents indexed."

    return f"""# ğŸ“Š Statistics

**ğŸ“š Documents:** {len(DOC_LIST)}
**ğŸ“„ Pages:** {len(set(m['page'] for m in METADATA))}
**ğŸ”¢ Chunks:** {len(METADATA)}

**Files:**
{chr(10).join(['â€¢ ' + doc for doc in DOC_LIST])}"""

# ==================== GRADIO UI ====================

css = """
.gradio-container {
    font-family: 'Inter', sans-serif;
    max-width: 1400px;
    margin: auto;
}
.header {
    background: linear-gradient(135deg, #667eea 0%, #764ba2 100%);
    padding: 30px;
    border-radius: 15px;
    color: white;
    text-align: center;
    margin-bottom: 25px;
}
"""

with gr.Blocks(css=css, theme=gr.themes.Soft(primary_hue="indigo"), title="StudyMate") as app:

    gr.HTML('<div class="header"><h1>ğŸ“š StudyMate AI</h1><p>Powered by IBM Granite 3.0 2B</p></div>')

    with gr.Accordion("âš™ï¸ System Instructions", open=False):
        system_prompt = gr.Textbox(
            label="ğŸ“ System Prompt",
            value="You are an educational assistant. Explain clearly with examples. Use headings, bullets, and code blocks.",
            lines=3
        )

    with gr.Tabs():

        # Upload Tab
        with gr.Tab("ğŸ“ Upload Documents"):
            gr.Markdown("### Upload PDFs")
            pdf_files = gr.File(label="Select PDFs", file_count="multiple", file_types=[".pdf"], type="filepath")
            index_btn = gr.Button("ğŸ” Index Documents", variant="primary", size="lg")
            index_status = gr.Textbox(label="Status", lines=8)
            stats = gr.Markdown(get_statistics())

            index_btn.click(index_documents, [pdf_files], [index_status]).then(get_statistics, [], [stats])

        # Q&A Tab
        with gr.Tab("ğŸ’¬ Ask Questions"):
            gr.Markdown("### Ask About Your Documents")

            with gr.Row():
                question = gr.Textbox(label="Question", placeholder="What is...?", lines=3, scale=3)
                difficulty = gr.Radio(["Beginner", "Intermediate", "Advanced"], value="Intermediate", label="Level", scale=1)

            ask_btn = gr.Button("ğŸš€ Get Answer", variant="primary", size="lg")

            with gr.Row():
                confidence = gr.Number(label="ğŸ¯ Confidence", scale=1)

            answer = gr.Textbox(label="ğŸ’¡ Answer", lines=12)

            with gr.Accordion("ğŸ“š Sources", open=False):
                citations = gr.Textbox(label="Citations", lines=6)

            ask_btn.click(answer_question, [question, difficulty, system_prompt], [answer, citations, confidence])

        # Notes Tab
        with gr.Tab("ğŸ“ Notes"):
            gr.Markdown("### Generate Study Notes")
            notes_topic = gr.Textbox(label="Topic", placeholder="Enter topic...", lines=2)
            notes_btn = gr.Button("ğŸ“ Generate", variant="primary", size="lg")
            notes_output = gr.Textbox(label="Notes", lines=15)

            with gr.Row():
                export_btn = gr.Button("ğŸ’¾ Export DOCX")
                export_file = gr.File(label="Download")

            notes_btn.click(generate_notes, [notes_topic, system_prompt], [notes_output])
            export_btn.click(export_notes_docx, [notes_output], [export_file])

        # Quiz Tab
        with gr.Tab("ğŸ“‹ Quiz"):
            gr.Markdown("### Generate Quiz")
            with gr.Row():
                quiz_topic = gr.Textbox(label="Topic", scale=3)
                quiz_num = gr.Slider(1, 15, 5, step=1, label="Questions", scale=1)

            quiz_btn = gr.Button("ğŸ“‹ Generate", variant="primary", size="lg")
            quiz_output = gr.Textbox(label="Quiz", lines=15)

            quiz_btn.click(generate_quiz, [quiz_topic, quiz_num, system_prompt], [quiz_output])

        # Flashcards Tab
        with gr.Tab("ğŸƒ Flashcards"):
            gr.Markdown("### Generate Flashcards")
            with gr.Row():
                card_topic = gr.Textbox(label="Topic", scale=3)
                card_num = gr.Slider(1, 25, 10, step=1, label="Cards", scale=1)

            card_btn = gr.Button("ğŸƒ Generate", variant="primary", size="lg")
            card_output = gr.Textbox(label="Flashcards", lines=15)

            card_btn.click(generate_flashcards, [card_topic, card_num], [card_output])

        # Search Tab
        with gr.Tab("ğŸ” Smart Search"):
            gr.Markdown("### Find Formulas, Tables, Definitions")
            search_query = gr.Textbox(label="Search", placeholder="formula, table, definition...", lines=2)
            search_btn = gr.Button("ğŸ” Search", variant="primary", size="lg")
            search_output = gr.Textbox(label="Results", lines=15)

            search_btn.click(smart_pdf_search, [search_query], [search_output])

        # TTS Tab
        with gr.Tab("ğŸ”Š Text-to-Speech"):
            gr.Markdown("### Convert Text to Audio")
            tts_text = gr.Textbox(label="Text", lines=6, placeholder="Paste text...")
            tts_lang = gr.Dropdown([("English", "en"), ("Hindi", "hi"), ("Spanish", "es")], value="en", label="Language")
            tts_btn = gr.Button("ğŸ”Š Generate", variant="primary", size="lg")
            tts_audio = gr.Audio(label="Audio")

            tts_btn.click(create_tts, [tts_text, tts_lang], [tts_audio])

    gr.Markdown("""
---
### ğŸ“– Instructions:
1. Upload PDFs â†’ Click Index
2. Use tabs for Q&A, Notes, Quiz, etc.
3. Powered by IBM Granite 3.0 2B
    """)

if __name__ == "__main__":
    app.launch(share=True)

# StudyMate - HACKATHON READY - IBM Granite 3.0 2B - NO ERRORS
# Install: !pip install -q pymupdf sentence-transformers faiss-cpu gradio transformers python-docx fpdf gTTS torch accelerate bitsandbytes

import os, re, json, tempfile, uuid, warnings
warnings.filterwarnings('ignore')
from typing import List, Dict, Any, Tuple
import fitz
import numpy as np
from sentence_transformers import SentenceTransformer
import faiss
import gradio as gr
from gtts import gTTS
from datetime import datetime
from docx import Document
from fpdf import FPDF
import torch
from transformers import AutoTokenizer, AutoModelForCausalLM

# ==================== CONFIGURATION ====================
print("ğŸš€ Loading StudyMate...")

EMBED_MODEL = SentenceTransformer("sentence-transformers/all-MiniLM-L6-v2")
print("âœ… Embedding model ready")

# IBM Granite 3.0 2B Instruct
GRANITE_MODEL = "ibm-granite/granite-3.0-2b-instruct"
granite_tokenizer = None
granite_model = None

try:
    print("â³ Loading IBM Granite 3.0 2B...")
    granite_tokenizer = AutoTokenizer.from_pretrained(GRANITE_MODEL, trust_remote_code=True)
    granite_model = AutoModelForCausalLM.from_pretrained(
        GRANITE_MODEL,
        device_map="auto",
        torch_dtype=torch.float16,
        trust_remote_code=True,
        low_cpu_mem_usage=True
    )
    print("âœ… IBM Granite loaded successfully!")
except Exception as e:
    print(f"âš ï¸ Granite error: {e}")

VECTOR_INDEX = None
METADATA = []
DOC_LIST = []

# ==================== PDF PROCESSING ====================

def extract_pdf_text(pdf_file, filename):
    """Extract text from PDF"""
    try:
        if isinstance(pdf_file, str):
            doc = fitz.open(pdf_file)
        else:
            pdf_bytes = pdf_file.read() if hasattr(pdf_file, 'read') else pdf_file
            doc = fitz.open(stream=pdf_bytes, filetype="pdf")

        chunks = []
        for pno in range(len(doc)):
            page = doc.load_page(pno)
            text = page.get_text("text")
            text = re.sub(r'\s+', ' ', text).strip()

            if len(text) < 20:
                continue

            # Split into sentences
            sentences = re.split(r'(?<=[.!?])\s+(?=[A-Z])', text)

            current = ""
            for sent in sentences:
                if len(current) + len(sent) < 500:
                    current += " " + sent
                else:
                    if len(current) > 40:
                        chunks.append({
                            'text': current.strip(),
                            'page': pno + 1,
                            'source': filename,
                            'chunk_id': str(uuid.uuid4())[:8]
                        })
                    current = sent

            if len(current) > 40:
                chunks.append({
                    'text': current.strip(),
                    'page': pno + 1,
                    'source': filename,
                    'chunk_id': str(uuid.uuid4())[:8]
                })

        doc.close()
        return chunks
    except Exception as e:
        print(f"Error: {e}")
        return []

def index_pdfs(files):
    """Index uploaded PDFs"""
    global VECTOR_INDEX, METADATA, DOC_LIST

    if not files:
        return "âŒ Upload PDF files first!"

    METADATA = []
    texts = []
    DOC_LIST = []

    for f in files:
        try:
            fname = os.path.basename(f.name if hasattr(f, 'name') else "doc.pdf")
            DOC_LIST.append(fname)
            chunks = extract_pdf_text(f, fname)

            for c in chunks:
                METADATA.append(c)
                texts.append(c['text'])
        except Exception as e:
            continue

    if not texts:
        return "âŒ No text found in PDFs"

    try:
        embeddings = EMBED_MODEL.encode(texts, show_progress_bar=False, convert_to_numpy=True)
        faiss.normalize_L2(embeddings)
        idx = faiss.IndexFlatIP(embeddings.shape[1])
        idx.add(embeddings)
        VECTOR_INDEX = idx

        return f"""âœ… **Success!**

ğŸ“š Files: {len(DOC_LIST)}
ğŸ“„ Pages: {len(set(m['page'] for m in METADATA))}
ğŸ”¢ Chunks: {len(texts)}

**Indexed:**
{chr(10).join(['â€¢ ' + d for d in DOC_LIST])}

Ready to use! ğŸ‰"""
    except Exception as e:
        return f"âŒ Error: {e}"

def search_docs(query, top_k=5):
    """Search indexed documents"""
    if not VECTOR_INDEX or not METADATA:
        return []

    try:
        q_emb = EMBED_MODEL.encode([query], convert_to_numpy=True)
        faiss.normalize_L2(q_emb)
        D, I = VECTOR_INDEX.search(q_emb, min(top_k, len(METADATA)))

        results = []
        for score, idx in zip(D[0], I[0]):
            if 0 <= idx < len(METADATA):
                results.append((METADATA[idx], float(score)))
        return results
    except:
        return []

# ==================== GRANITE MODEL ====================

def granite_generate(prompt, system_prompt, max_tokens=1024):
    """Generate response using Granite"""
    if not granite_model or not granite_tokenizer:
        return "âš ï¸ Model not loaded\n\n" + prompt[:500]

    try:
        messages = [
            {"role": "system", "content": system_prompt},
            {"role": "user", "content": prompt}
        ]

        chat = granite_tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)
        inputs = granite_tokenizer(chat, return_tensors="pt").to(granite_model.device)

        with torch.no_grad():
            outputs = granite_model.generate(
                **inputs,
                max_new_tokens=max_tokens,
                temperature=0.7,
                top_p=0.9,
                do_sample=True,
                pad_token_id=granite_tokenizer.eos_token_id
            )

        response = granite_tokenizer.decode(outputs[0][inputs['input_ids'].shape[1]:], skip_special_tokens=True)
        return response.strip()
    except Exception as e:
        return f"Error: {e}"

# ==================== FEATURES ====================

def qa_answer(question, difficulty, system_prompt):
    """Q&A Feature"""
    if not question.strip():
        return "â“ Enter a question", "", 0

    if not VECTOR_INDEX:
        return "ğŸ“š **No PDFs indexed!**\n\nUpload PDFs in 'Upload Documents' tab first.", "", 0

    results = search_docs(question, 5)
    if not results:
        return "âŒ No relevant info found", "", 0

    context_parts = []
    citations = []

    for i, (meta, score) in enumerate(results):
        context_parts.append(f"[Source {i+1}] {meta['text']}")
        citations.append(f"""**ğŸ“„ Source {i+1}** (Relevance: {score:.1%})
â€¢ File: {meta['source']}
â€¢ Page: {meta['page']}
â€¢ Text: "{meta['text'][:200]}..."
""")

    context = "\n\n".join(context_parts)

    difficulty_map = {
        "Beginner": "Explain simply with examples. No jargon.",
        "Intermediate": "Balanced explanation with some technical details.",
        "Advanced": "Detailed technical explanation with advanced concepts."
    }

    sys_prompt = f"""{system_prompt}

{difficulty_map.get(difficulty, "")}

Format with:
# Headings
â€¢ Bullets
â€¢ Code blocks with ```
â€¢ Step-by-step"""

    prompt = f"""Context:
{context}

Question: {question}

Answer based ONLY on context above."""

    answer = granite_generate(prompt, sys_prompt)
    confidence = int(results[0][1] * 100)

    final = f"""# ğŸ“ Answer

{answer}

---
*IBM Granite 3.0 2B | Confidence: {confidence}%*"""

    return final, "\n".join(citations), confidence

def make_notes(topic, system_prompt):
    """Notes Feature"""
    if not topic.strip():
        return "â“ Enter topic"

    if not VECTOR_INDEX:
        return "ğŸ“š No PDFs indexed"

    results = search_docs(topic, 8)
    if not results:
        return "âŒ No info found"

    context = "\n\n".join([f"[{i+1}] {m['text']}" for i, (m, _) in enumerate(results)])

    prompt = f"""Create study notes on: {topic}

Context:
{context}

Format:
# Main Topic
## Subtopics
â€¢ Key points
â€¢ Examples"""

    notes = granite_generate(prompt, system_prompt, 1500)
    sources = set([m['source'] for m, _ in results])

    return f"""{notes}

---
**ğŸ“š Sources:** {', '.join(sources)}"""

def make_quiz(topic, num_q, system_prompt):
    """Quiz Feature - FIXED"""
    if not topic.strip():
        return "â“ Enter topic"

    if not VECTOR_INDEX:
        return "ğŸ“š No PDFs indexed"

    try:
        results = search_docs(topic, 10)
        if not results:
            return "âŒ No info found"

        context = "\n\n".join([m['text'] for m, _ in results[:num_q]])

        prompt = f"""Generate {num_q} MCQs about: {topic}

Context:
{context}

Format each:
**Q1. [Question text]**
A) Option A
B) Option B
C) Option C
D) Option D
âœ“ Answer: [Letter]
ğŸ“ Why: [Explanation]

Make questions clear and educational."""

        quiz = granite_generate(prompt, f"{system_prompt}\n\nCreate clear quiz questions.", 1500)
        return quiz
    except Exception as e:
        return f"âŒ Error: {str(e)}\n\nTry different topic or check PDFs."

def make_flashcards(topic, num_cards):
    """Flashcards Feature"""
    if not topic.strip():
        return "â“ Enter topic"

    if not VECTOR_INDEX:
        return "ğŸ“š No PDFs indexed"

    results = search_docs(topic, num_cards)
    if not results:
        return "âŒ No info found"

    cards = f"# ğŸƒ Flashcards: {topic}\n\n"

    for i, (meta, score) in enumerate(results[:num_cards], 1):
        text = meta['text']
        sents = text.split('.')

        front = sents[0].strip()[:150] if sents else text[:150]
        back = '. '.join(sents[1:3]).strip()[:300] if len(sents) > 1 else text[:300]

        cards += f"""---
### Card {i}

**ğŸ”¹ Front:**
{front}

**ğŸ”¹ Back:**
{back}

*{meta['source']}, p.{meta['page']}*

"""
    return cards

def smart_search(query):
    """Smart Search - FIXED"""
    if not METADATA:
        return "ğŸ“š No PDFs indexed"

    if not query.strip():
        return "â“ Enter search query"

    try:
        q_lower = query.lower()
        results = []

        for meta in METADATA:
            text = meta['text']
            matched = False
            match_type = "Keyword"

            # Formula search
            if 'formula' in q_lower or 'equation' in q_lower:
                if any(c in text for c in ['=', 'âˆ‘', 'âˆ«', 'âˆ‚', 'â‰ˆ']):
                    matched = True
                    match_type = "Formula"

            # Table search
            elif 'table' in q_lower:
                if re.search(r'\btable\b|\bcolumn\b|\brow\b', text, re.I):
                    matched = True
                    match_type = "Table"

            # Definition search
            elif 'definition' in q_lower or 'define' in q_lower:
                if re.search(r'\bdefined as\b|\bdefinition\b|\bmeans\b', text, re.I):
                    matched = True
                    match_type = "Definition"

            # General keyword
            elif q_lower in text.lower():
                matched = True

            if matched:
                results.append(f"""**ğŸ“„ {meta['source']}** (Page {meta['page']}) - *{match_type}*
> {text[:250]}...
""")

        if not results:
            return f"âŒ No matches for '{query}'\n\nTry: 'formula', 'table', 'definition' or keywords"

        return f"""# ğŸ” Search Results: {query}

Found {len(results)} matches:

{chr(10).join(results[:15])}"""
    except Exception as e:
        return f"âŒ Error: {str(e)}\n\nTry simpler search terms."

def make_tts(text, lang):
    """TTS Feature"""
    if not text.strip():
        return None
    try:
        tts = gTTS(text=text[:1000], lang=lang)
        tmp = tempfile.NamedTemporaryFile(delete=False, suffix=".mp3")
        tts.save(tmp.name)
        return tmp.name
    except:
        return None

def export_docx(text):
    """Export to DOCX"""
    if not text.strip():
        return None
    try:
        doc = Document()
        doc.add_heading('StudyMate Notes', 0)
        doc.add_paragraph(text)
        path = os.path.join(tempfile.gettempdir(), f"Notes_{datetime.now().strftime('%Y%m%d_%H%M%S')}.docx")
        doc.save(path)
        return path
    except:
        return None

def stats():
    """Statistics"""
    if not METADATA:
        return "ğŸ“Š No PDFs indexed yet"

    return f"""# ğŸ“Š Statistics

**ğŸ“š Documents:** {len(DOC_LIST)}
**ğŸ“„ Pages:** {len(set(m['page'] for m in METADATA))}
**ğŸ”¢ Chunks:** {len(METADATA)}

**Files:** {', '.join(DOC_LIST)}"""

# ==================== GRADIO UI ====================

css = """
.gradio-container {font-family: 'Inter', sans-serif; max-width: 1400px; margin: auto;}
.header {background: linear-gradient(135deg, #667eea 0%, #764ba2 100%); padding: 35px;
border-radius: 15px; color: white; text-align: center; margin-bottom: 25px; box-shadow: 0 10px 30px rgba(102,126,234,0.3);}
.header h1 {margin: 0; font-size: 2.5em; font-weight: 700;}
.header p {margin: 10px 0 0 0; font-size: 1.1em; opacity: 0.95;}
"""

with gr.Blocks(css=css, theme=gr.themes.Soft(primary_hue="indigo"), title="StudyMate") as app:

    gr.HTML('<div class="header"><h1>ğŸ“š StudyMate AI</h1><p>IBM Granite 3.0 2B Instruct | Hackathon Edition</p></div>')

    with gr.Accordion("âš™ï¸ System Instructions", open=False):
        sys_prompt = gr.Textbox(
            label="ğŸ“ System Prompt",
            value="You are an educational assistant. Explain clearly with examples. Use headings, bullet points, and code blocks for formatting.",
            lines=3
        )

    with gr.Tabs():

        # Upload
        with gr.Tab("ğŸ“ Upload Documents"):
            gr.Markdown("### ğŸ“¤ Upload Your Study Materials (PDFs)")
            pdfs = gr.File(label="Select PDF Files", file_count="multiple", file_types=[".pdf"], type="filepath")
            idx_btn = gr.Button("ğŸ” Index Documents", variant="primary", size="lg")
            idx_status = gr.Textbox(label="ğŸ“‹ Status", lines=10)
            stats_md = gr.Markdown(stats())

            idx_btn.click(index_pdfs, [pdfs], [idx_status]).then(stats, [], [stats_md])

        # Q&A
        with gr.Tab("ğŸ’¬ Ask Questions"):
            gr.Markdown("### ğŸ¯ Ask Questions About Your Documents")

            with gr.Row():
                question = gr.Textbox(label="â“ Your Question", placeholder="What is machine learning?", lines=3, scale=3)
                difficulty = gr.Radio(["Beginner", "Intermediate", "Advanced"], value="Intermediate", label="ğŸ“Š Level", scale=1)

            ask_btn = gr.Button("ğŸš€ Get Answer", variant="primary", size="lg")
            confidence = gr.Number(label="ğŸ¯ Confidence %", scale=1)
            answer = gr.Textbox(label="ğŸ’¡ Answer", lines=12)

            with gr.Accordion("ğŸ“š View Sources", open=False):
                citations = gr.Textbox(label="Citations", lines=6)

            ask_btn.click(qa_answer, [question, difficulty, sys_prompt], [answer, citations, confidence])

        # Notes
        with gr.Tab("ğŸ“ Generate Notes"):
            gr.Markdown("### ğŸ“– Create Study Notes")
            notes_topic = gr.Textbox(label="ğŸ“Œ Topic", placeholder="Machine learning algorithms", lines=2)
            notes_btn = gr.Button("ğŸ“ Generate Notes", variant="primary", size="lg")
            notes_out = gr.Textbox(label="ğŸ“„ Notes", lines=15)

            with gr.Row():
                export_btn = gr.Button("ğŸ’¾ Export to DOCX")
                export_file = gr.File(label="ğŸ“¥ Download")

            notes_btn.click(make_notes, [notes_topic, sys_prompt], [notes_out])
            export_btn.click(export_docx, [notes_out], [export_file])

        # Quiz
        with gr.Tab("ğŸ“‹ Generate Quiz"):
            gr.Markdown("### ğŸ“ Create Practice Quiz")
            with gr.Row():
                quiz_topic = gr.Textbox(label="ğŸ“Œ Topic", placeholder="Neural networks", scale=3)
                quiz_num = gr.Slider(1, 15, 5, step=1, label="ğŸ”¢ Questions", scale=1)

            quiz_btn = gr.Button("ğŸ“‹ Generate Quiz", variant="primary", size="lg")
            quiz_out = gr.Textbox(label="ğŸ“ Quiz", lines=15)

            quiz_btn.click(make_quiz, [quiz_topic, quiz_num, sys_prompt], [quiz_out])

        # Flashcards
        with gr.Tab("ğŸƒ Flashcards"):
            gr.Markdown("### ğŸ´ Generate Flashcards")
            with gr.Row():
                card_topic = gr.Textbox(label="ğŸ“Œ Topic", placeholder="Deep learning concepts", scale=3)
                card_num = gr.Slider(1, 25, 10, step=1, label="ğŸ”¢ Cards", scale=1)

            card_btn = gr.Button("ğŸƒ Generate Flashcards", variant="primary", size="lg")
            card_out = gr.Textbox(label="ğŸ´ Flashcards", lines=15)

            card_btn.click(make_flashcards, [card_topic, card_num], [card_out])

        # Smart Search
        with gr.Tab("ğŸ” Smart Search"):
            gr.Markdown("### ğŸ” Find Formulas, Tables, Definitions")
            gr.Markdown("*Try: 'formula', 'table', 'definition' or any keyword*")

            search_q = gr.Textbox(label="ğŸ” Search Query", placeholder="formula", lines=2)
            search_btn = gr.Button("ğŸ” Search", variant="primary", size="lg")
            search_out = gr.Textbox(label="ğŸ“Š Results", lines=15)

            search_btn.click(smart_search, [search_q], [search_out])

        # TTS
        with gr.Tab("ğŸ”Š Text-to-Speech"):
            gr.Markdown("### ğŸ§ Convert Text to Audio")
            tts_text = gr.Textbox(label="ğŸ“ Text", lines=6, placeholder="Paste text here...")
            tts_lang = gr.Dropdown([("English", "en"), ("Hindi", "hi"), ("Spanish", "es"), ("French", "fr")], value="en", label="ğŸŒ Language")
            tts_btn = gr.Button("ğŸ”Š Generate Audio", variant="primary", size="lg")
            tts_audio = gr.Audio(label="ğŸµ Audio")

            tts_btn.click(make_tts, [tts_text, tts_lang], [tts_audio])

    gr.Markdown("""
---
### ğŸš€ Quick Start:
1. **Upload PDFs** â†’ Click "Index Documents"
2. Use tabs for **Q&A, Notes, Quiz, Flashcards, Search, TTS**
3. Powered by **IBM Granite 3.0 2B Instruct**

**All features working! Good luck in the hackathon! ğŸ†**
    """)

if __name__ == "__main__":
    app.launch(share=True, server_name="0.0.0.0")

# 1. Install (just once)
!pip install -q pymupdf sentence-transformers faiss-cpu gradio transformers python-docx fpdf gTTS torch accelerate

# 2. Copy the ENTIRE code above
# 3. Run it
# 4. Upload PDFs â†’ Index â†’ Use features!

# StudyMate - HACKATHON READY - IBM Granite 3.0 2B - NO ERRORS
# Install: !pip install -q pymupdf sentence-transformers faiss-cpu gradio transformers python-docx fpdf gTTS torch accelerate bitsandbytes

import os, re, json, tempfile, uuid, warnings
warnings.filterwarnings('ignore')
from typing import List, Dict, Any, Tuple
import fitz
import numpy as np
from sentence_transformers import SentenceTransformer
import faiss
import gradio as gr
from gtts import gTTS
from datetime import datetime
from docx import Document
from fpdf import FPDF
import torch
from transformers import AutoTokenizer, AutoModelForCausalLM

# ==================== CONFIGURATION ====================
print("ğŸš€ Loading StudyMate...")

EMBED_MODEL = SentenceTransformer("sentence-transformers/all-MiniLM-L6-v2")
print("âœ… Embedding model ready")

# IBM Granite 3.0 2B Instruct
GRANITE_MODEL = "ibm-granite/granite-3.0-2b-instruct"
granite_tokenizer = None
granite_model = None

try:
    print("â³ Loading IBM Granite 3.0 2B...")
    granite_tokenizer = AutoTokenizer.from_pretrained(GRANITE_MODEL, trust_remote_code=True)
    granite_model = AutoModelForCausalLM.from_pretrained(
        GRANITE_MODEL,
        device_map="auto",
        torch_dtype=torch.float16,
        trust_remote_code=True,
        low_cpu_mem_usage=True
    )
    print("âœ… IBM Granite loaded successfully!")
except Exception as e:
    print(f"âš ï¸ Granite error: {e}")

VECTOR_INDEX = None
METADATA = []
DOC_LIST = []

# ==================== PDF PROCESSING ====================

def extract_pdf_text(pdf_file, filename):
    """Extract text from PDF"""
    try:
        if isinstance(pdf_file, str):
            doc = fitz.open(pdf_file)
        else:
            pdf_bytes = pdf_file.read() if hasattr(pdf_file, 'read') else pdf_file
            doc = fitz.open(stream=pdf_bytes, filetype="pdf")

        chunks = []
        for pno in range(len(doc)):
            page = doc.load_page(pno)
            text = page.get_text("text")
            text = re.sub(r'\s+', ' ', text).strip()

            if len(text) < 20:
                continue

            # Split into sentences
            sentences = re.split(r'(?<=[.!?])\s+(?=[A-Z])', text)

            current = ""
            for sent in sentences:
                if len(current) + len(sent) < 500:
                    current += " " + sent
                else:
                    if len(current) > 40:
                        chunks.append({
                            'text': current.strip(),
                            'page': pno + 1,
                            'source': filename,
                            'chunk_id': str(uuid.uuid4())[:8]
                        })
                    current = sent

            if len(current) > 40:
                chunks.append({
                    'text': current.strip(),
                    'page': pno + 1,
                    'source': filename,
                    'chunk_id': str(uuid.uuid4())[:8]
                })

        doc.close()
        return chunks
    except Exception as e:
        print(f"Error: {e}")
        return []

def index_pdfs(files):
    """Index uploaded PDFs"""
    global VECTOR_INDEX, METADATA, DOC_LIST

    if not files:
        return "âŒ Upload PDF files first!"

    METADATA = []
    texts = []
    DOC_LIST = []

    for f in files:
        try:
            fname = os.path.basename(f.name if hasattr(f, 'name') else "doc.pdf")
            DOC_LIST.append(fname)
            chunks = extract_pdf_text(f, fname)

            for c in chunks:
                METADATA.append(c)
                texts.append(c['text'])
        except Exception as e:
            continue

    if not texts:
        return "âŒ No text found in PDFs"

    try:
        embeddings = EMBED_MODEL.encode(texts, show_progress_bar=False, convert_to_numpy=True)
        faiss.normalize_L2(embeddings)
        idx = faiss.IndexFlatIP(embeddings.shape[1])
        idx.add(embeddings)
        VECTOR_INDEX = idx

        return f"""âœ… **Success!**

ğŸ“š Files: {len(DOC_LIST)}
ğŸ“„ Pages: {len(set(m['page'] for m in METADATA))}
ğŸ”¢ Chunks: {len(texts)}

**Indexed:**
{chr(10).join(['â€¢ ' + d for d in DOC_LIST])}

Ready to use! ğŸ‰"""
    except Exception as e:
        return f"âŒ Error: {e}"

def search_docs(query, top_k=5):
    """Search indexed documents"""
    if not VECTOR_INDEX or not METADATA:
        return []

    try:
        q_emb = EMBED_MODEL.encode([query], convert_to_numpy=True)
        faiss.normalize_L2(q_emb)
        D, I = VECTOR_INDEX.search(q_emb, min(top_k, len(METADATA)))

        results = []
        for score, idx in zip(D[0], I[0]):
            if 0 <= idx < len(METADATA):
                results.append((METADATA[idx], float(score)))
        return results
    except:
        return []

# ==================== GRANITE MODEL ====================

def granite_generate(prompt, system_prompt, max_tokens=1024):
    """Generate response using Granite"""
    if not granite_model or not granite_tokenizer:
        return "âš ï¸ Model not loaded\n\n" + prompt[:500]

    try:
        messages = [
            {"role": "system", "content": system_prompt},
            {"role": "user", "content": prompt}
        ]

        chat = granite_tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)
        inputs = granite_tokenizer(chat, return_tensors="pt").to(granite_model.device)

        with torch.no_grad():
            outputs = granite_model.generate(
                **inputs,
                max_new_tokens=max_tokens,
                temperature=0.7,
                top_p=0.9,
                do_sample=True,
                pad_token_id=granite_tokenizer.eos_token_id
            )

        response = granite_tokenizer.decode(outputs[0][inputs['input_ids'].shape[1]:], skip_special_tokens=True)
        return response.strip()
    except Exception as e:
        return f"Error: {e}"

# ==================== FEATURES ====================

def qa_answer(question, difficulty, system_prompt):
    """Q&A Feature"""
    if not question.strip():
        return "â“ Enter a question", "", 0

    if not VECTOR_INDEX:
        return "ğŸ“š **No PDFs indexed!**\n\nUpload PDFs in 'Upload Documents' tab first.", "", 0

    results = search_docs(question, 5)
    if not results:
        return "âŒ No relevant info found", "", 0

    context_parts = []
    citations = []

    for i, (meta, score) in enumerate(results):
        context_parts.append(f"[Source {i+1}] {meta['text']}")
        citations.append(f"""**ğŸ“„ Source {i+1}** (Relevance: {score:.1%})
â€¢ File: {meta['source']}
â€¢ Page: {meta['page']}
â€¢ Text: "{meta['text'][:200]}..."
""")

    context = "\n\n".join(context_parts)

    difficulty_map = {
        "Beginner": "Explain simply with examples. No jargon.",
        "Intermediate": "Balanced explanation with some technical details.",
        "Advanced": "Detailed technical explanation with advanced concepts."
    }

    sys_prompt = f"""{system_prompt}

{difficulty_map.get(difficulty, "")}

Format with:
# Headings
â€¢ Bullets
â€¢ Code blocks with ```
â€¢ Step-by-step"""

    prompt = f"""Context:
{context}

Question: {question}

Answer based ONLY on context above."""

    answer = granite_generate(prompt, sys_prompt)
    confidence = int(results[0][1] * 100)

    final = f"""# ğŸ“ Answer

{answer}

---
*IBM Granite 3.0 2B | Confidence: {confidence}%*"""

    return final, "\n".join(citations), confidence

def make_notes(topic, system_prompt):
    """Notes Feature"""
    if not topic.strip():
        return "â“ Enter topic"

    if not VECTOR_INDEX:
        return "ğŸ“š No PDFs indexed"

    results = search_docs(topic, 8)
    if not results:
        return "âŒ No info found"

    context = "\n\n".join([f"[{i+1}] {m['text']}" for i, (m, _) in enumerate(results)])

    prompt = f"""Create study notes on: {topic}

Context:
{context}

Format:
# Main Topic
## Subtopics
â€¢ Key points
â€¢ Examples"""

    notes = granite_generate(prompt, system_prompt, 1500)
    sources = set([m['source'] for m, _ in results])

    return f"""{notes}

---
**ğŸ“š Sources:** {', '.join(sources)}"""

def make_quiz(topic, num_q, system_prompt):
    """Quiz Feature - FIXED"""
    if not topic.strip():
        return "â“ Enter topic"

    if not VECTOR_INDEX:
        return "ğŸ“š No PDFs indexed"

    try:
        results = search_docs(topic, 10)
        if not results:
            return "âŒ No info found"

        context = "\n\n".join([m['text'] for m, _ in results[:num_q]])

        prompt = f"""Generate {num_q} MCQs about: {topic}

Context:
{context}

Format each:
**Q1. [Question text]**
A) Option A
B) Option B
C) Option C
D) Option D
âœ“ Answer: [Letter]
ğŸ“ Why: [Explanation]

Make questions clear and educational."""

        quiz = granite_generate(prompt, f"{system_prompt}\n\nCreate clear quiz questions.", 1500)
        return quiz
    except Exception as e:
        return f"âŒ Error: {str(e)}\n\nTry different topic or check PDFs."

def make_flashcards(topic, num_cards):
    """Flashcards Feature"""
    if not topic.strip():
        return "â“ Enter topic"

    if not VECTOR_INDEX:
        return "ğŸ“š No PDFs indexed"

    results = search_docs(topic, num_cards)
    if not results:
        return "âŒ No info found"

    cards = f"# ğŸƒ Flashcards: {topic}\n\n"

    for i, (meta, score) in enumerate(results[:num_cards], 1):
        text = meta['text']
        sents = text.split('.')

        front = sents[0].strip()[:150] if sents else text[:150]
        back = '. '.join(sents[1:3]).strip()[:300] if len(sents) > 1 else text[:300]

        cards += f"""---
### Card {i}

**ğŸ”¹ Front:**
{front}

**ğŸ”¹ Back:**
{back}

*{meta['source']}, p.{meta['page']}*

"""
    return cards

def smart_search(query):
    """Smart Search - FIXED"""
    if not METADATA:
        return "ğŸ“š No PDFs indexed"

    if not query.strip():
        return "â“ Enter search query"

    try:
        q_lower = query.lower()
        results = []

        for meta in METADATA:
            text = meta['text']
            matched = False
            match_type = "Keyword"

            # Formula search
            if 'formula' in q_lower or 'equation' in q_lower:
                if any(c in text for c in ['=', 'âˆ‘', 'âˆ«', 'âˆ‚', 'â‰ˆ']):
                    matched = True
                    match_type = "Formula"

            # Table search
            elif 'table' in q_lower:
                if re.search(r'\btable\b|\bcolumn\b|\brow\b', text, re.I):
                    matched = True
                    match_type = "Table"

            # Definition search
            elif 'definition' in q_lower or 'define' in q_lower:
                if re.search(r'\bdefined as\b|\bdefinition\b|\bmeans\b', text, re.I):
                    matched = True
                    match_type = "Definition"

            # General keyword
            elif q_lower in text.lower():
                matched = True

            if matched:
                results.append(f"""**ğŸ“„ {meta['source']}** (Page {meta['page']}) - *{match_type}*
> {text[:250]}...
""")

        if not results:
            return f"âŒ No matches for '{query}'\n\nTry: 'formula', 'table', 'definition' or keywords"

        return f"""# ğŸ” Search Results: {query}

Found {len(results)} matches:

{chr(10).join(results[:15])}"""
    except Exception as e:
        return f"âŒ Error: {str(e)}\n\nTry simpler search terms."

def make_tts(text, lang):
    """TTS Feature"""
    if not text.strip():
        return None
    try:
        tts = gTTS(text=text[:1000], lang=lang)
        tmp = tempfile.NamedTemporaryFile(delete=False, suffix=".mp3")
        tts.save(tmp.name)
        return tmp.name
    except:
        return None

def export_docx(text):
    """Export to DOCX"""
    if not text.strip():
        return None
    try:
        doc = Document()
        doc.add_heading('StudyMate Notes', 0)
        doc.add_paragraph(text)
        path = os.path.join(tempfile.gettempdir(), f"Notes_{datetime.now().strftime('%Y%m%d_%H%M%S')}.docx")
        doc.save(path)
        return path
    except:
        return None

def stats():
    """Statistics"""
    if not METADATA:
        return "ğŸ“Š No PDFs indexed yet"

    return f"""# ğŸ“Š Statistics

**ğŸ“š Documents:** {len(DOC_LIST)}
**ğŸ“„ Pages:** {len(set(m['page'] for m in METADATA))}
**ğŸ”¢ Chunks:** {len(METADATA)}

**Files:** {', '.join(DOC_LIST)}"""

# ==================== GRADIO UI ====================

css = """
.gradio-container {font-family: 'Inter', sans-serif; max-width: 1400px; margin: auto;}
.header {background: linear-gradient(135deg, #667eea 0%, #764ba2 100%); padding: 35px;
border-radius: 15px; color: white; text-align: center; margin-bottom: 25px; box-shadow: 0 10px 30px rgba(102,126,234,0.3);}
.header h1 {margin: 0; font-size: 2.5em; font-weight: 700;}
.header p {margin: 10px 0 0 0; font-size: 1.1em; opacity: 0.95;}
"""

with gr.Blocks(css=css, theme=gr.themes.Soft(primary_hue="indigo"), title="StudyMate") as app:

    gr.HTML('<div class="header"><h1>ğŸ“š StudyMate AI</h1><p>IBM Granite 3.0 2B Instruct | Hackathon Edition</p></div>')

    with gr.Accordion("âš™ï¸ System Instructions", open=False):
        sys_prompt = gr.Textbox(
            label="ğŸ“ System Prompt",
            value="You are an educational assistant. Explain clearly with examples. Use headings, bullet points, and code blocks for formatting.",
            lines=3
        )

    with gr.Tabs():

        # Upload
        with gr.Tab("ğŸ“ Upload Documents"):
            gr.Markdown("### ğŸ“¤ Upload Your Study Materials (PDFs)")
            pdfs = gr.File(label="Select PDF Files", file_count="multiple", file_types=[".pdf"], type="filepath")
            idx_btn = gr.Button("ğŸ” Index Documents", variant="primary", size="lg")
            idx_status = gr.Textbox(label="ğŸ“‹ Status", lines=10)
            stats_md = gr.Markdown(stats())

            idx_btn.click(index_pdfs, [pdfs], [idx_status]).then(stats, [], [stats_md])

        # Q&A
        with gr.Tab("ğŸ’¬ Ask Questions"):
            gr.Markdown("### ğŸ¯ Ask Questions About Your Documents")

            with gr.Row():
                question = gr.Textbox(label="â“ Your Question", placeholder="What is machine learning?", lines=3, scale=3)
                difficulty = gr.Radio(["Beginner", "Intermediate", "Advanced"], value="Intermediate", label="ğŸ“Š Level", scale=1)

            ask_btn = gr.Button("ğŸš€ Get Answer", variant="primary", size="lg")
            confidence = gr.Number(label="ğŸ¯ Confidence %", scale=1)
            answer = gr.Textbox(label="ğŸ’¡ Answer", lines=12)

            with gr.Accordion("ğŸ“š View Sources", open=False):
                citations = gr.Textbox(label="Citations", lines=6)

            ask_btn.click(qa_answer, [question, difficulty, sys_prompt], [answer, citations, confidence])

        # Notes
        with gr.Tab("ğŸ“ Generate Notes"):
            gr.Markdown("### ğŸ“– Create Study Notes")
            notes_topic = gr.Textbox(label="ğŸ“Œ Topic", placeholder="Machine learning algorithms", lines=2)
            notes_btn = gr.Button("ğŸ“ Generate Notes", variant="primary", size="lg")
            notes_out = gr.Textbox(label="ğŸ“„ Notes", lines=15)

            with gr.Row():
                export_btn = gr.Button("ğŸ’¾ Export to DOCX")
                export_file = gr.File(label="ğŸ“¥ Download")

            notes_btn.click(make_notes, [notes_topic, sys_prompt], [notes_out])
            export_btn.click(export_docx, [notes_out], [export_file])

        # Quiz
        with gr.Tab("ğŸ“‹ Generate Quiz"):
            gr.Markdown("### ğŸ“ Create Practice Quiz")
            with gr.Row():
                quiz_topic = gr.Textbox(label="ğŸ“Œ Topic", placeholder="Neural networks", scale=3)
                quiz_num = gr.Slider(1, 15, 5, step=1, label="ğŸ”¢ Questions", scale=1)

            quiz_btn = gr.Button("ğŸ“‹ Generate Quiz", variant="primary", size="lg")
            quiz_out = gr.Textbox(label="ğŸ“ Quiz", lines=15)

            quiz_btn.click(make_quiz, [quiz_topic, quiz_num, sys_prompt], [quiz_out])

        # Flashcards
        with gr.Tab("ğŸƒ Flashcards"):
            gr.Markdown("### ğŸ´ Generate Flashcards")
            with gr.Row():
                card_topic = gr.Textbox(label="ğŸ“Œ Topic", placeholder="Deep learning concepts", scale=3)
                card_num = gr.Slider(1, 25, 10, step=1, label="ğŸ”¢ Cards", scale=1)

            card_btn = gr.Button("ğŸƒ Generate Flashcards", variant="primary", size="lg")
            card_out = gr.Textbox(label="ğŸ´ Flashcards", lines=15)

            card_btn.click(make_flashcards, [card_topic, card_num], [card_out])

        # Smart Search
        with gr.Tab("ğŸ” Smart Search"):
            gr.Markdown("### ğŸ” Find Formulas, Tables, Definitions")
            gr.Markdown("*Try: 'formula', 'table', 'definition' or any keyword*")

            search_q = gr.Textbox(label="ğŸ” Search Query", placeholder="formula", lines=2)
            search_btn = gr.Button("ğŸ” Search", variant="primary", size="lg")
            search_out = gr.Textbox(label="ğŸ“Š Results", lines=15)

            search_btn.click(smart_search, [search_q], [search_out])

        # TTS
        with gr.Tab("ğŸ”Š Text-to-Speech"):
            gr.Markdown("### ğŸ§ Convert Text to Audio")
            tts_text = gr.Textbox(label="ğŸ“ Text", lines=6, placeholder="Paste text here...")
            tts_lang = gr.Dropdown([("English", "en"), ("Hindi", "hi"), ("Spanish", "es"), ("French", "fr")], value="en", label="ğŸŒ Language")
            tts_btn = gr.Button("ğŸ”Š Generate Audio", variant="primary", size="lg")
            tts_audio = gr.Audio(label="ğŸµ Audio")

            tts_btn.click(make_tts, [tts_text, tts_lang], [tts_audio])

    gr.Markdown("""
---
### ğŸš€ Quick Start:
1. **Upload PDFs** â†’ Click "Index Documents"
2. Use tabs for **Q&A, Notes, Quiz, Flashcards, Search, TTS**
3. Powered by **IBM Granite 3.0 2B Instruct**

**All features working! Good luck in the hackathon! ğŸ†**
    """)

if __name__ == "__main__":
    app.launch(share=True, server_name="0.0.0.0")

# ========================================
# StudyMate - COMPLETE GOOGLE COLAB VERSION
# Copy this ENTIRE cell and run in Colab
# ========================================

# Install dependencies
print("ğŸ“¦ Installing dependencies...")
!pip install -q pymupdf sentence-transformers faiss-cpu gradio transformers python-docx fpdf gTTS torch accelerate bitsandbytes

# Imports
import os, re, json, tempfile, uuid, warnings
warnings.filterwarnings('ignore')
from typing import List, Dict, Any, Tuple
import fitz
import numpy as np
from sentence_transformers import SentenceTransformer
import faiss
import gradio as gr
from gtts import gTTS
from datetime import datetime
from docx import Document
from fpdf import FPDF
import torch
from transformers import AutoTokenizer, AutoModelForCausalLM

print("âœ… Imports complete")

# Load Models
print("ğŸ”„ Loading embedding model...")
EMBED_MODEL = SentenceTransformer("sentence-transformers/all-MiniLM-L6-v2")
print("âœ… Embedding model ready")

print("â³ Loading IBM Granite 3.0 2B Instruct...")
GRANITE_MODEL = "ibm-granite/granite-3.0-2b-instruct"
granite_tokenizer = None
granite_model = None

try:
    granite_tokenizer = AutoTokenizer.from_pretrained(GRANITE_MODEL, trust_remote_code=True)
    granite_model = AutoModelForCausalLM.from_pretrained(
        GRANITE_MODEL,
        device_map="auto",
        torch_dtype=torch.float16,
        trust_remote_code=True,
        low_cpu_mem_usage=True
    )
    print("âœ… IBM Granite loaded!")
except Exception as e:
    print(f"âš ï¸ Granite error: {e}")

# Global variables
VECTOR_INDEX = None
METADATA = []
DOC_LIST = []

# PDF Processing
def extract_pdf_text(pdf_file, filename):
    try:
        if isinstance(pdf_file, str):
            doc = fitz.open(pdf_file)
        else:
            pdf_bytes = pdf_file.read() if hasattr(pdf_file, 'read') else pdf_file
            doc = fitz.open(stream=pdf_bytes, filetype="pdf")

        chunks = []
        for pno in range(len(doc)):
            page = doc.load_page(pno)
            text = page.get_text("text")
            text = re.sub(r'\s+', ' ', text).strip()

            if len(text) < 20:
                continue

            sentences = re.split(r'(?<=[.!?])\s+(?=[A-Z])', text)
            current = ""

            for sent in sentences:
                if len(current) + len(sent) < 500:
                    current += " " + sent
                else:
                    if len(current) > 40:
                        chunks.append({
                            'text': current.strip(),
                            'page': pno + 1,
                            'source': filename,
                            'chunk_id': str(uuid.uuid4())[:8]
                        })
                    current = sent

            if len(current) > 40:
                chunks.append({
                    'text': current.strip(),
                    'page': pno + 1,
                    'source': filename,
                    'chunk_id': str(uuid.uuid4())[:8]
                })

        doc.close()
        return chunks
    except Exception as e:
        print(f"Error: {e}")
        return []

def index_pdfs(files):
    global VECTOR_INDEX, METADATA, DOC_LIST

    if not files:
        return "âŒ Upload PDF files first!"

    METADATA = []
    texts = []
    DOC_LIST = []

    for f in files:
        try:
            fname = os.path.basename(f.name if hasattr(f, 'name') else "doc.pdf")
            DOC_LIST.append(fname)
            chunks = extract_pdf_text(f, fname)

            for c in chunks:
                METADATA.append(c)
                texts.append(c['text'])
        except Exception as e:
            continue

    if not texts:
        return "âŒ No text found in PDFs"

    try:
        embeddings = EMBED_MODEL.encode(texts, show_progress_bar=False, convert_to_numpy=True)
        faiss.normalize_L2(embeddings)
        idx = faiss.IndexFlatIP(embeddings.shape[1])
        idx.add(embeddings)
        VECTOR_INDEX = idx

        return f"""âœ… **Indexing Complete!**

ğŸ“š Files: {len(DOC_LIST)}
ğŸ“„ Pages: {len(set(m['page'] for m in METADATA))}
ğŸ”¢ Chunks: {len(texts)}

**Indexed Files:**
{chr(10).join(['â€¢ ' + d for d in DOC_LIST])}

âœ¨ Ready to use!"""
    except Exception as e:
        return f"âŒ Error: {e}"

def search_docs(query, top_k=5):
    if not VECTOR_INDEX or not METADATA:
        return []

    try:
        q_emb = EMBED_MODEL.encode([query], convert_to_numpy=True)
        faiss.normalize_L2(q_emb)
        D, I = VECTOR_INDEX.search(q_emb, min(top_k, len(METADATA)))

        results = []
        for score, idx in zip(D[0], I[0]):
            if 0 <= idx < len(METADATA):
                results.append((METADATA[idx], float(score)))
        return results
    except:
        return []

# Granite Generation
def granite_generate(prompt, system_prompt, max_tokens=1024):
    if not granite_model or not granite_tokenizer:
        return "âš ï¸ Model not loaded\n\n" + prompt[:500]

    try:
        messages = [
            {"role": "system", "content": system_prompt},
            {"role": "user", "content": prompt}
        ]

        chat = granite_tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)
        inputs = granite_tokenizer(chat, return_tensors="pt").to(granite_model.device)

        with torch.no_grad():
            outputs = granite_model.generate(
                **inputs,
                max_new_tokens=max_tokens,
                temperature=0.7,
                top_p=0.9,
                do_sample=True,
                pad_token_id=granite_tokenizer.eos_token_id
            )

        response = granite_tokenizer.decode(outputs[0][inputs['input_ids'].shape[1]:], skip_special_tokens=True)
        return response.strip()
    except Exception as e:
        return f"Error: {e}"

# Features
def qa_answer(question, difficulty, system_prompt):
    if not question.strip():
        return "â“ Enter a question", "", 0

    if not VECTOR_INDEX:
        return "ğŸ“š **No PDFs indexed!**\n\nUpload PDFs in 'Upload Documents' tab first.", "", 0

    results = search_docs(question, 5)
    if not results:
        return "âŒ No relevant info found", "", 0

    context_parts = []
    citations = []

    for i, (meta, score) in enumerate(results):
        context_parts.append(f"[Source {i+1}] {meta['text']}")
        citations.append(f"""**ğŸ“„ Source {i+1}** (Relevance: {score:.1%})
â€¢ File: {meta['source']}
â€¢ Page: {meta['page']}
â€¢ Text: "{meta['text'][:200]}..."
""")

    context = "\n\n".join(context_parts)

    difficulty_map = {
        "Beginner": "Explain simply with examples. No jargon.",
        "Intermediate": "Balanced explanation with some technical details.",
        "Advanced": "Detailed technical explanation with advanced concepts."
    }

    sys_prompt = f"""{system_prompt}

{difficulty_map.get(difficulty, "")}

Format with:
# Headings
â€¢ Bullets
â€¢ Code blocks with ```
â€¢ Step-by-step"""

    prompt = f"""Context:
{context}

Question: {question}

Answer based ONLY on context above."""

    answer = granite_generate(prompt, sys_prompt)
    confidence = int(results[0][1] * 100)

    final = f"""# ğŸ“ Answer

{answer}

---
*IBM Granite 3.0 2B | Confidence: {confidence}%*"""

    return final, "\n".join(citations), confidence

def make_notes(topic, system_prompt):
    if not topic.strip():
        return "â“ Enter topic"

    if not VECTOR_INDEX:
        return "ğŸ“š No PDFs indexed"

    results = search_docs(topic, 8)
    if not results:
        return "âŒ No info found"

    context = "\n\n".join([f"[{i+1}] {m['text']}" for i, (m, _) in enumerate(results)])

    prompt = f"""Create study notes on: {topic}

Context:
{context}

Format:
# Main Topic
## Subtopics
â€¢ Key points
â€¢ Examples"""

    notes = granite_generate(prompt, system_prompt, 1500)
    sources = set([m['source'] for m, _ in results])

    return f"""{notes}

---
**ğŸ“š Sources:** {', '.join(sources)}"""

def make_quiz(topic, num_q, system_prompt):
    if not topic.strip():
        return "â“ Enter topic"

    if not VECTOR_INDEX:
        return "ğŸ“š No PDFs indexed"

    try:
        results = search_docs(topic, 10)
        if not results:
            return "âŒ No info found"

        context = "\n\n".join([m['text'] for m, _ in results[:num_q]])

        prompt = f"""Generate {num_q} MCQs about: {topic}

Context:
{context}

Format each:
**Q1. [Question text]**
A) Option A
B) Option B
C) Option C
D) Option D
âœ“ Answer: [Letter]
ğŸ“ Why: [Explanation]

Make questions clear and educational."""

        quiz = granite_generate(prompt, f"{system_prompt}\n\nCreate clear quiz questions.", 1500)
        return quiz
    except Exception as e:
        return f"âŒ Error: {str(e)}"

def make_flashcards(topic, num_cards):
    if not topic.strip():
        return "â“ Enter topic"

    if not VECTOR_INDEX:
        return "ğŸ“š No PDFs indexed"

    results = search_docs(topic, num_cards)
    if not results:
        return "âŒ No info found"

    cards = f"# ğŸƒ Flashcards: {topic}\n\n"

    for i, (meta, score) in enumerate(results[:num_cards], 1):
        text = meta['text']
        sents = text.split('.')

        front = sents[0].strip()[:150] if sents else text[:150]
        back = '. '.join(sents[1:3]).strip()[:300] if len(sents) > 1 else text[:300]

        cards += f"""---
### Card {i}

**ğŸ”¹ Front:**
{front}

**ğŸ”¹ Back:**
{back}

*{meta['source']}, p.{meta['page']}*

"""
    return cards

def smart_search(query):
    if not METADATA:
        return "ğŸ“š No PDFs indexed"

    if not query.strip():
        return "â“ Enter search query"

    try:
        q_lower = query.lower()
        results = []

        for meta in METADATA:
            text = meta['text']
            matched = False
            match_type = "Keyword"

            if 'formula' in q_lower or 'equation' in q_lower:
                if any(c in text for c in ['=', 'âˆ‘', 'âˆ«', 'âˆ‚', 'â‰ˆ']):
                    matched = True
                    match_type = "Formula"
            elif 'table' in q_lower:
                if re.search(r'\btable\b|\bcolumn\b|\brow\b', text, re.I):
                    matched = True
                    match_type = "Table"
            elif 'definition' in q_lower or 'define' in q_lower:
                if re.search(r'\bdefined as\b|\bdefinition\b|\bmeans\b', text, re.I):
                    matched = True
                    match_type = "Definition"
            elif q_lower in text.lower():
                matched = True

            if matched:
                results.append(f"""**ğŸ“„ {meta['source']}** (Page {meta['page']}) - *{match_type}*
> {text[:250]}...
""")

        if not results:
            return f"âŒ No matches for '{query}'"

        return f"""# ğŸ” Search Results: {query}

Found {len(results)} matches:

{chr(10).join(results[:15])}"""
    except Exception as e:
        return f"âŒ Error: {str(e)}"

def make_tts(text, lang):
    if not text.strip():
        return None
    try:
        tts = gTTS(text=text[:1000], lang=lang)
        tmp = tempfile.NamedTemporaryFile(delete=False, suffix=".mp3")
        tts.save(tmp.name)
        return tmp.name
    except:
        return None

def export_docx(text):
    if not text.strip():
        return None
    try:
        doc = Document()
        doc.add_heading('StudyMate Notes', 0)
        doc.add_paragraph(text)
        path = os.path.join(tempfile.gettempdir(), f"Notes_{datetime.now().strftime('%Y%m%d_%H%M%S')}.docx")
        doc.save(path)
        return path
    except:
        return None

def stats():
    if not METADATA:
        return "ğŸ“Š No PDFs indexed yet"

    return f"""# ğŸ“Š Statistics

**ğŸ“š Documents:** {len(DOC_LIST)}
**ğŸ“„ Pages:** {len(set(m['page'] for m in METADATA))}
**ğŸ”¢ Chunks:** {len(METADATA)}

**Files:** {', '.join(DOC_LIST)}"""

# Gradio UI
css = """
.gradio-container {font-family: 'Inter', sans-serif; max-width: 1400px; margin: auto;}
.header {background: linear-gradient(135deg, #667eea 0%, #764ba2 100%); padding: 35px;
border-radius: 15px; color: white; text-align: center; margin-bottom: 25px; box-shadow: 0 10px 30px rgba(102,126,234,0.3);}
.header h1 {margin: 0; font-size: 2.5em; font-weight: 700;}
.header p {margin: 10px 0 0 0; font-size: 1.1em; opacity: 0.95;}
"""

with gr.Blocks(css=css, theme=gr.themes.Soft(primary_hue="indigo"), title="StudyMate") as app:

    gr.HTML('<div class="header"><h1>ğŸ“š StudyMate AI</h1><p>IBM Granite 3.0 2B Instruct | Hackathon Edition</p></div>')

    with gr.Accordion("âš™ï¸ System Instructions", open=False):
        sys_prompt = gr.Textbox(
            label="ğŸ“ System Prompt",
            value="You are an educational assistant. Explain clearly with examples. Use headings, bullet points, and code blocks for formatting.",
            lines=3
        )

    with gr.Tabs():

        with gr.Tab("ğŸ“ Upload Documents"):
            gr.Markdown("### ğŸ“¤ Upload Your Study Materials (PDFs)")
            pdfs = gr.File(label="Select PDF Files", file_count="multiple", file_types=[".pdf"], type="filepath")
            idx_btn = gr.Button("ğŸ” Index Documents", variant="primary", size="lg")
            idx_status = gr.Textbox(label="ğŸ“‹ Status", lines=10)
            stats_md = gr.Markdown(stats())

            idx_btn.click(index_pdfs, [pdfs], [idx_status]).then(stats, [], [stats_md])

        with gr.Tab("ğŸ’¬ Ask Questions"):
            gr.Markdown("### ğŸ¯ Ask Questions About Your Documents")

            with gr.Row():
                question = gr.Textbox(label="â“ Your Question", placeholder="What is machine learning?", lines=3, scale=3)
                difficulty = gr.Radio(["Beginner", "Intermediate", "Advanced"], value="Intermediate", label="ğŸ“Š Level", scale=1)

            ask_btn = gr.Button("ğŸš€ Get Answer", variant="primary", size="lg")
            confidence = gr.Number(label="ğŸ¯ Confidence %", scale=1)
            answer = gr.Textbox(label="ğŸ’¡ Answer", lines=12)

            with gr.Accordion("ğŸ“š View Sources", open=False):
                citations = gr.Textbox(label="Citations", lines=6)

            ask_btn.click(qa_answer, [question, difficulty, sys_prompt], [answer, citations, confidence])

        with gr.Tab("ğŸ“ Generate Notes"):
            gr.Markdown("### ğŸ“– Create Study Notes")
            notes_topic = gr.Textbox(label="ğŸ“Œ Topic", placeholder="Machine learning algorithms", lines=2)
            notes_btn = gr.Button("ğŸ“ Generate Notes", variant="primary", size="lg")
            notes_out = gr.Textbox(label="ğŸ“„ Notes", lines=15)

            with gr.Row():
                export_btn = gr.Button("ğŸ’¾ Export to DOCX")
                export_file = gr.File(label="ğŸ“¥ Download")

            notes_btn.click(make_notes, [notes_topic, sys_prompt], [notes_out])
            export_btn.click(export_docx, [notes_out], [export_file])

        with gr.Tab("ğŸ“‹ Generate Quiz"):
            gr.Markdown("### ğŸ“ Create Practice Quiz")
            with gr.Row():
                quiz_topic = gr.Textbox(label="ğŸ“Œ Topic", placeholder="Neural networks", scale=3)
                quiz_num = gr.Slider(1, 15, 5, step=1, label="ğŸ”¢ Questions", scale=1)

            quiz_btn = gr.Button("ğŸ“‹ Generate Quiz", variant="primary", size="lg")
            quiz_out = gr.Textbox(label="ğŸ“ Quiz", lines=15)

            quiz_btn.click(make_quiz, [quiz_topic, quiz_num, sys_prompt], [quiz_out])

        with gr.Tab("ğŸƒ Flashcards"):
            gr.Markdown("### ğŸ´ Generate Flashcards")
            with gr.Row():
                card_topic = gr.Textbox(label="ğŸ“Œ Topic", placeholder="Deep learning concepts", scale=3)
                card_num = gr.Slider(1, 25, 10, step=1, label="ğŸ”¢ Cards", scale=1)

            card_btn = gr.Button("ğŸƒ Generate Flashcards", variant="primary", size="lg")
            card_out = gr.Textbox(label="ğŸ´ Flashcards", lines=15)

            card_btn.click(make_flashcards, [card_topic, card_num], [card_out])

        with gr.Tab("ğŸ” Smart Search"):
            gr.Markdown("### ğŸ” Find Formulas, Tables, Definitions")
            search_q = gr.Textbox(label="ğŸ” Search Query", placeholder="formula", lines=2)
            search_btn = gr.Button("ğŸ” Search", variant="primary", size="lg")
            search_out = gr.Textbox(label="ğŸ“Š Results", lines=15)

            search_btn.click(smart_search, [search_q], [search_out])

        with gr.Tab("ğŸ”Š Text-to-Speech"):
            gr.Markdown("### ğŸ§ Convert Text to Audio")
            tts_text = gr.Textbox(label="ğŸ“ Text", lines=6, placeholder="Paste text here...")
            tts_lang = gr.Dropdown([("English", "en"), ("Hindi", "hi"), ("Spanish", "es"), ("French", "fr")], value="en", label="ğŸŒ Language")
            tts_btn = gr.Button("ğŸ”Š Generate Audio", variant="primary", size="lg")
            tts_audio = gr.Audio(label="ğŸµ Audio")

            tts_btn.click(make_tts, [tts_text, tts_lang], [tts_audio])

    gr.Markdown("""
---
### ğŸš€ Quick Start:
1. **Upload PDFs** â†’ Click "Index Documents"
2. Use tabs for **Q&A, Notes, Quiz, Flashcards, Search, TTS**
3. Powered by **IBM Granite 3.0 2B Instruct**

**ğŸ† All features working! Good luck!**
    """)

# Launch for Google Colab
print("\nğŸš€ Launching StudyMate...")
print("â³ Please wait for the public URL...")
app.launch(debug=True, share=True)